\chapter{相关技术与理论基础}

\section{前端技术选型}
客户端采用 React 作为 UI 框架，配合 TypeScript 进行静态类型检查，使用 Vite 完成项目构建与模块热替换。React 的组件化特性使得"对话窗口""学习进度看板""写作反馈面板"等界面单元能够独立开发与复用。TypeScript 的类型系统有助于前后端在接口定义上保持一致，减少因字段命名或类型不匹配导致的运行时错误。为适配企业微信内嵌 WebView 与普通浏览器两种接入场景，界面布局采用弹性栅格与媒体查询相结合的方式，同时在功能层面通过特性检测判断宿主环境能力，保障跨端体验基本一致。

\section{后端服务架构}
后端选用 Go 语言与 Gin 框架，前者在并发模型与部署体积上具备优势，后者提供中间件机制便于横切关注点（日志、限流、异常恢复）的统一处理。代码组织遵循分层原则：Handler 层负责请求解析与响应封装；Service 层承载业务逻辑；Repository 层封装数据库操作。权限模型基于角色（RBAC），教师、助教、学生对同一资源拥有差异化访问范围；认证方案采用 JWT，令牌无状态特性简化了分布式环境下的会话管理。持久化层使用 MySQL，表结构变更通过迁移脚本版本化管理，便于开发、测试与生产环境保持同步。

\section{大语言模型与检索增强}

\subsection{Transformer 架构简述}
Transformer 模型\cite{vaswani2017}以自注意力机制取代传统循环结构，在处理长距离依赖时表现出明显优势。当前主流大语言模型（GPT 系列、Qwen 系列等）均以 Transformer 为骨干网络，在语言理解、摘要生成、翻译等任务上取得显著成效。不过，这类模型并非总能给出可靠答案：有时会产生与事实不符的描述（即"幻觉"），有时会省略推导中间步骤，给教学场景带来可信度风险。

\subsection{检索增强生成的基本思路}
检索增强生成（Retrieval-Augmented Generation，简称 RAG）\cite{lewis2020}的核心想法是在生成前先从外部知识库中检索相关片段，再将其拼接到上下文中供模型参考。相比纯参数记忆，这种做法能有效降低幻觉概率并提升结论的可追溯性。常规 RAG 流程包括：文档切片、向量化、相似度检索、上下文拼接。但在理工科推导型场景下，纯向量检索难以表达公式间的推导链条与概念间的前置依赖关系，召回质量会受到限制。

\subsection{GraphRAG 的设计考量}
GraphRAG 在向量检索基础上叠加知识图谱结构，将文档片段组织为"节点—边"的索引。其运作方式可归纳为三点：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{混合检索}：同时执行关键词匹配与语义向量检索，通过倒数排名融合（RRF）合并两路结果，兼顾精准召回与语义扩展。
  \item \textbf{图扩展}：以检索到的片段为种子，沿图谱边扩展一至两跳，补充前置概念与关联推导要点，形成更完整的推理材料。
  \item \textbf{引用标注}：生成答案时以 \texttt{[1][2]} 等编号标注来源片段，便于用户核对原文。
\end{enumerate}

\section{技能系统与提示编排}
在多课程、多任务场景下，不同功能点对提示词有差异化需求。如果在代码各处零散编写提示模板，后期维护成本会急剧上升。因此本系统将"提示模板、上下文注入规则、输出格式约束、安全边界"统一封装为"技能"（Skill），并以注册表形式集中管理。

\subsection{技能抽象}
每个技能的核心由三部分组成：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{系统提示}：描述角色身份、任务边界、输出结构（例如先给出结论再展开解释、必须标注引用来源、遇到代写请求应婉拒）。
  \item \textbf{上下文注入}：在不干扰用户原始对话的前提下，向模型上下文追加课程背景、作业要求、学生档案等信息。
  \item \textbf{扩展注册}：通过注册表统一管理技能集合，对历史模式名提供别名映射以保持接口兼容。
\end{enumerate}

\subsection{技能与其他能力的组合}
技能侧重"教学风格与行为约束"，检索增强侧重"证据支撑"，工具调用侧重"可验证计算"。三者在工程上相互独立：同一技能既可在无检索场景下工作，也可在启用 GraphRAG 时以证据片段约束输出。这种解耦设计降低了功能扩展时的耦合风险，具体路由与实现细节留待第4章展开。

\section{工具调用机制}
工具调用（Tool Calling）使语言模型能够判断何时需要外部工具协助，并输出结构化调用请求。调用完成后，工具返回值被回注到对话上下文，模型再据此生成最终回答。该机制在可验证性要求较高的教学任务中尤为重要：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{符号计算}：调用计算库对公式进行推导或化简。
  \item \textbf{数值仿真}：调用仿真服务执行课程专属计算（如电磁场分布）。
  \item \textbf{表达式求值}：对物理常数或中间结果进行精确计算，规避模型"心算"失误。
  \item \textbf{规则检查}：对写作样本的字数、结构元素、引用格式等执行规则化校验。
\end{enumerate}
为防止模型无限循环调用或调用未授权工具，系统在工程层面实施工具白名单、最大调用次数与调用超时等策略，第4章将结合具体实现说明。

\section{引导式学习与学习状态追踪}
传统的"一问一答"模式难以覆盖复杂能力的渐进式培养。引导式学习将某一学习主题拆分为 3--6 个可管理的步骤，系统依次向学生抛出问题或提示，待完成当前步骤后再进入下一步。这种"苏格拉底式"的互动方式有助于减少"听懂但不会做"的落差。

为支撑这一模式，系统需显式维护会话状态：当前步骤、已完成步骤、历史对话、阶段性薄弱点等。这些信息进一步汇聚为学习档案（课程画像与跨课程全局画像），供教师端查看学情分布或开展个性化干预。会话管理与薄弱点检测的具体实现将在第4章详细讨论。

\section{异构加速：NPU 与 FPGA}
在教育场景下，模型推理与检索服务需要兼顾响应延迟与部署成本。除传统 GPU 方案外，NPU 与 FPGA 两种异构加速路径可分别针对大模型推理与向量检索提供优化。

\subsection{NPU 加速大模型推理}
NPU（神经网络处理器）针对矩阵运算与张量操作进行硬件层面优化，配合 CANN 软件栈实现算子加速。以华为 Ascend 910B 为例，7B 参数量模型的首 token 延迟与 NVIDIA RTX 4090 处于同一数量级，但满载功耗降低约 20\%，适合对能耗与机房条件有约束的校园部署场景。系统在设计上预留了 NPU 推理接入能力：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{模型转换}：通过 ATC 工具将 Hugging Face 模型转换为 Ascend 可执行格式（OM 模型）。
  \item \textbf{推理框架}：使用 MindIE 或 vLLM Ascend 后端提供推理接口。
  \item \textbf{服务封装}：对外暴露 OpenAI-compatible 接口，AI 服务层无需区分上游硬件。
\end{enumerate}

\subsection{FPGA 加速向量检索}
在检索增强链路中，Embedding 计算是主要的延迟来源之一。当 GPU 被大模型推理占用时，Embedding 与 Reranker 若共享 GPU 会产生资源竞争，影响整体吞吐。FPGA（现场可编程门阵列）作为可定制化加速器，适合承载固定计算模式的推理任务。

\textbf{FPGA 加速的基本思路}：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{模型量化与编译}：将 Embedding 模型（如 bge-base 或 sentence-transformers）通过 Vitis AI 量化为 INT8 精度，并编译为 FPGA 可执行指令（xmodel）。
  \item \textbf{硬件部署}：在 Xilinx Alveo 系列加速卡（如 U50、U250）上部署量化模型，通过 PCIE 与主机通信。
  \item \textbf{服务封装}：将 FPGA 推理封装为 HTTP 微服务，对外提供与 CPU/GPU Embedding 服务相同的接口，便于 AI 服务层无感切换。
\end{enumerate}

\textbf{预期收益}：
\begin{enumerate}[label=(\arabic*)]
  \item \textbf{延迟降低}：FPGA 的确定性时序特性可提供稳定的低延迟 Embedding 计算。
  \item \textbf{资源解耦}：将 Embedding 从 GPU 迁移至 FPGA，释放 GPU 资源专注于大模型推理，提升整体并发能力。
  \item \textbf{能效优势}：FPGA 典型功耗 30--75W，远低于 GPU，适合持续运行的检索服务。
\end{enumerate}

第4章将给出 FPGA 加速 Embedding 服务的具体实现，第5章将对比 CPU、GPU、FPGA 三种 Embedding 后端的性能与检索质量。

\section{参数高效微调与训练链路}
教育场景对输出稳定性要求较高，仅靠提示编排有时难以满足需求。参数高效微调提供了一条低成本路径。

\subsection{LoRA 与 QLoRA}
LoRA\cite{hu2021lora}在注意力层引入低秩适配器，仅训练约 0.1\% 的参数即可调整模型行为分布；QLoRA\cite{dettmers2023qlora}在此基础上叠加量化技术，使单张消费级 GPU 也能完成 7B 量级模型的微调。训练产物以 adapter 形式保存，可快速切换或回滚。

\subsection{数据蒸馏与质量门禁}
在启动微调前，系统先对训练数据执行"蒸馏 + smoke 验证"：蒸馏阶段将 chat-style 样本提取为易于审阅的格式并统计去重率；smoke 阶段在分钟级别输出轻量度量（样本数、字段覆盖率、困惑度分布等），用于发现空样本、字段缺失或重复激增等问题。该流程不用于宣称模型最终效果，而是充当训练链路的"自检关卡"。

\subsection{回归评测}
持续迭代需要可量化的回归机制。系统维护一份固定回归集，覆盖 tutor、grader、guided\_learning、tool、rag 等任务类型。每次训练后执行预测与评测脚本，输出引用一致性、工具调用准确率、结构化输出可解析率等指标，并附带典型案例分析。第5章将给出具体评测设计。

\section{数值仿真与可视化}
对于"计算与现象解释"类需求（以电磁场课程为例），系统可扩展数值仿真模块。采用有限差分法（FDM）求解 Laplace 或 Poisson 方程，典型场景包括二维静电场分布、同轴线电容场等。Python 科学计算生态（NumPy、SciPy、Matplotlib）提供数值求解与绘图能力。仿真服务通过 FastAPI 暴露 HTTP 接口，支持参数化任务提交与异步执行，返回数据（JSON）与图像（PNG）两种形式。仿真结果可作为可验证证据注入 AI 回答，形成"计算—解释—理解"的闭环。该模块属于"课程专属能力"，与写作类课程的规则检查工具形成互补。

\section{本章小结}
本章介绍了系统涉及的关键技术与理论背景：前端采用 React + TypeScript + Vite 构建跨端客户端；后端基于 Go + Gin 实现业务服务与权限治理；异构加速层支持 GPU、NPU 与 FPGA 三种后端——NPU 用于大模型推理的低功耗部署，FPGA 用于 Embedding 服务的资源解耦与延迟优化；GraphRAG 在向量检索基础上引入图扩展以提升召回质量；技能系统统一管理提示编排；工具调用为可验证计算提供支撑；引导式学习与学习档案为过程性辅导提供状态基础；LoRA/QLoRA 与数据蒸馏、回归评测构成后训练链路。上述技术共同支撑平台在不同课程中的可迁移落地，后续章节将围绕需求分析、系统实现与测试评估逐步展开。
