# 论文翻译 (Translations)

本目录包含重要学术论文的中文翻译文档，便于深入理解和学习。

## 翻译文档列表

### attention-is-all-you-need-full.md
- **原论文**: Attention Is All You Need (Vaswani et al., 2017)
- **翻译状态**: 完整翻译
- **主要内容**: 
  - Transformer架构详解
  - 自注意力机制原理
  - 多头注意力实现
  - 位置编码方法
  - 训练技巧和优化策略

### retrieval-augmented-generation-full.md
- **原论文**: Retrieval-Augmented Generation (Lewis et al., 2020)
- **翻译状态**: 完整翻译
- **主要内容**:
  - RAG架构设计
  - 检索器和生成器结合
  - 知识密集型任务处理
  - 端到端训练方法
  - 实验结果分析

### paper-translations.md
- **类型**: 翻译索引文档
- **内容**: 所有翻译文档的汇总和导航

## 翻译原则

### 准确性
- 保持原文的技术准确性
- 专业术语使用标准译名
- 重要概念提供英文对照

### 可读性
- 符合中文表达习惯
- 适当调整句式结构
- 添加必要的解释说明

### 完整性
- 包含所有重要章节
- 保留关键公式和图表
- 提供参考文献翻译

## 术语对照表

| 英文术语 | 中文译名 | 说明 |
|---------|---------|------|
| Attention | 注意力 | 注意力机制 |
| Transformer | 变换器 | 一种神经网络架构 |
| Self-Attention | 自注意力 | 序列内部的注意力计算 |
| Multi-Head Attention | 多头注意力 | 并行的多个注意力头 |
| Retrieval-Augmented Generation | 检索增强生成 | RAG技术 |
| Knowledge-Intensive | 知识密集型 | 需要大量外部知识的任务 |

## 使用建议

### 学习路径
1. 先阅读翻译文档了解整体思路
2. 对照原文学习专业术语
3. 重点理解核心算法和公式
4. 思考在项目中的应用方式

### 深入研究
- 查阅相关的后续研究
- 分析不同实现方案
- 实践关键算法
- 总结应用经验

## 更新维护

- 定期检查原论文更新
- 修正翻译中的错误
- 补充最新的研究进展
- 添加实践应用案例