# Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks（全文翻译）

## 题名
Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks

## 作者
Patrick Lewis、Ethan Perez、Aleksandra Piktus、Fabio Petroni、Vladimir Karpukhin、Naman Goyal、Heinrich Küttler、Mike Lewis、Wen-tau Yih、Tim Rocktäschel、Sebastian Riedel、Douwe Kiela

## 摘要
大规模预训练语言模型能够在参数中存储事实知识，在下游任务微调后取得很强性能。然而，这类模型在知识访问与精确操控方面仍有限，因此在知识密集型任务上通常落后于任务特定架构。此外，给出模型决策的可追溯证据与动态更新模型知识仍是开放问题。此前将可微分外部记忆引入预训练模型的工作，多集中于抽取式任务。本文提出一种通用的检索增强生成（RAG）微调框架，将参数化记忆与非参数记忆结合用于生成。我们构建的 RAG 模型以预训练 seq2seq 模型作为参数化记忆，以维基百科稠密向量索引作为非参数记忆，并用预训练神经检索器访问。我们比较两种 RAG 形式：一种在整段生成中使用同一检索文档，另一种允许每个 token 使用不同文档。我们在多个知识密集型任务上微调并评估 RAG，在三项开放域问答任务上达到最优结果，优于纯参数化 seq2seq 与检索-抽取式架构。在生成任务上，RAG 生成的文本更具体、更丰富且更符合事实。

## 1 引言
预训练语言模型已被证明能够从数据中学习大量知识，即使不访问外部记忆，也可作为参数化的隐式知识库。然而，这类模型难以更新知识、难以给出可解释的证据，且可能产生“幻觉”。将参数化记忆与非参数（检索式）记忆结合的混合模型可缓解上述问题，因为知识可直接更新，且检索到的证据可被检查。REALM 与 ORQA 等模型在开放域抽取式问答中取得良好结果，但尚未扩展到通用生成模型。

本文将混合记忆引入“序列到序列（seq2seq）模型”这一 NLP 主力架构，提出检索增强生成（RAG）。RAG 将预训练检索器（DPR）与预训练生成器（BART）组合为可端到端微调的概率模型。检索器给出与输入相关的潜在文档，生成器在这些文档与输入的上下文下生成输出。我们提出 RAG-Sequence 与 RAG-Token 两种边缘化方式，既可用于任何 seq2seq 任务，也可联合训练检索器与生成器。

我们的结果显示，混合参数化与非参数记忆对于知识密集型任务具有显著优势：在开放域问答上达到最优，在知识密集型生成任务上更具体与真实，并展示了通过替换非参数索引更新模型知识的能力。

## 2 方法
我们研究 RAG 模型：给定输入序列 $x$，检索文本片段 $z$，并在生成目标序列 $y$ 时将其作为额外上下文。模型包含两部分：(i) 检索器 $p_\eta(z|x)$；(ii) 生成器 $p_\theta(y_i|x,z,y_{1:i-1})$。

### 2.1 模型
**RAG-Sequence**：使用同一检索文档生成整个序列，将文档作为单一潜变量边缘化，使用 top-$K$ 近似：

$$
 p_{\text{RAG-Sequence}}(y|x) \approx \sum_{z\in\text{top-}k(p(\cdot|x))} p_\eta(z|x)\,p_\theta(y|x,z)
$$

其中 $p_\theta(y|x,z)=\prod_i p_\theta(y_i|x,z,y_{1:i-1})$。

**RAG-Token**：每个 token 可使用不同检索文档，对每步输出进行边缘化：

$$
 p_{\text{RAG-Token}}(y|x) \approx \prod_i \sum_{z\in\text{top-}k(p(\cdot|x))} p_\eta(z|x)\,p_\theta(y_i|x,z,y_{1:i-1})
$$

当将类别视为长度为 1 的序列时，RAG-Sequence 与 RAG-Token 等价，可用于分类任务。

### 2.2 检索器：DPR
检索器 $p_\eta(z|x)$ 采用 DPR 的双编码器结构：

$$
 p_\eta(z|x) \propto \exp(d(z)^T q(x))
$$

其中 $d(z)=\text{BERT}_d(z)$ 为文档编码器输出，$q(x)=\text{BERT}_q(x)$ 为查询编码器输出。检索 top-$k$ 文档为最大内积搜索（MIPS），可近似求解。我们使用 DPR 预训练检索器初始化并构建文档索引（非参数记忆）。

### 2.3 生成器：BART
生成器可为任意编码器—解码器结构，我们使用 BART-large。生成时将输入 $x$ 与检索片段 $z$ 进行拼接。BART 以去噪目标预训练，在多种生成任务上表现优秀。本文将 BART 参数视为参数化记忆。

### 2.4 训练
我们对检索器与生成器进行端到端联合训练，以最小化负对数似然 $-\log p(y|x)$。为避免频繁更新文档索引带来的开销，我们固定文档编码器与索引，仅微调查询编码器与 BART。

### 2.5 解码
**RAG-Token**：可视为标准自回归模型，用边缘化后的 token 概率进行 beam search。

**RAG-Sequence**：因 $p(y|x)$ 不分解为标准逐 token 形式，需对每个候选文档独立进行 beam search，并对不同文档下的概率进行边缘化。我们提出“Thorough Decoding”（为所有候选文档重新前向计算）与更高效的“Fast Decoding”（若某文档 beam 中未生成候选序列，则近似其概率为 0）。

## 3 实验设置
我们使用 2018 年 12 月维基百科构建非参数知识库，将每篇文章切分为 100 词片段，共约 2100 万文档。用 DPR 文档编码器计算向量并用 FAISS 构建 MIPS 索引，使用 HNSW 近似。训练时检索 top-$k$ 文档（$k\in\{5,10\}$），测试时 $k$ 由验证集确定。

### 3.1 开放域问答
将问题-答案视为 $(x,y)$ 对，直接最小化答案的负对数似然。比较对象包括抽取式问答与纯参数化“闭卷”模型。评测数据集：Natural Questions、TriviaQA、WebQuestions、CuratedTrec。WQ 与 CT 采用 NQ 训练初始化。TriviaQA 同时报告公开 Web Dev 测试集与 Wiki 测试集结果。

### 3.2 抽象式问答
使用 MSMARCO NLG 任务（v2.1），仅使用问题与答案，不使用给定检索段落，将其视为开放域抽象式问答。由于部分问题无法仅凭维基百科回答，RAG 可利用参数化知识补全。

### 3.3 Jeopardy 问题生成
以答案实体为输入，生成 Jeopardy 风格问题（事实描述式问题）。使用 SearchQA 划分：10 万训练、1.4 万验证、2.7 万测试。评测指标为 Q-BLEU-1，并进行人工评估（事实性与特异性）。

### 3.4 事实验证（FEVER）
将 claim 映射为三分类（支持/反驳/信息不足）或二分类任务，输出视为单 token。不同于多数方法，本文不使用证据监督；仅基于 claim-标签训练。评估指标为标签准确率。

## 4 结果

### 4.1 开放域问答
RAG 在四个开放域 QA 任务上均达到最优（TQA 使用与 T5 可比的 split 时），结合了生成灵活性与检索证据优势。RAG 仅使用 DPR 初始化的检索器即可获得强结果，不需要重排序器或抽取式阅读器。生成式回答还能利用“包含线索但不含答案原文”的文档，这是抽取式方法难以做到的。

### 4.2 抽象式问答
在 MSMARCO NLG 上，RAG 生成更具体、更符合事实，优于仅参数化的 BART 基线。

### 4.3 Jeopardy 问题生成
RAG 在 Q-BLEU-1 与人工评估上均优于 BART，生成更事实、更具体的问题。

### 4.4 事实验证
RAG 在 FEVER 任务上取得与强检索监督管线模型接近的性能，并展示了检索—生成框架在分类任务中的有效性。

### 4.5 额外结果与消融
- **生成多样性**：计算不同模型生成的 distinct n-gram 比例。RAG-Sequence 的多样性高于 RAG-Token，且两者均显著高于 BART。
- **检索消融**：冻结检索器会降低各任务性能，说明可学习检索有效。
- **BM25 对比**：在 FEVER 上 BM25 表现更好（可能因实体词重叠强），但在其他任务上可微检索优势明显。
- **索引热替换**：使用 2016 与 2018 维基索引验证知识更新能力，替换索引即可更新世界知识，无需再训练。
- **检索文档数量影响**：RAG-Sequence 随检索文档数量增加性能单调提升；RAG-Token 在 10 个文档附近达到峰值。

表格与图示数值较多，以下保留原文数值顺序列表（格式可能与原表略有差异）。

- 表 1：开放域 QA 测试结果（NQ/TQA/WQ/CT），含闭卷模型、检索模型与 RAG。
- 表 2：生成与分类测试结果（MSMARCO、Jeopardy QGen、FEVER）。
- 表 4：Jeopardy 任务人工评估（事实性与特异性）。
- 表 5：distinct tri-gram 比例（多样性）。
- 表 6：检索消融与 BM25 对比（多任务 EM/BLEU/Rouge/F1）。

## 5 相关工作
**单任务检索**：检索在开放域问答、事实核查、事实补全、长文本问答、维基生成、对话、翻译、语言建模等任务中能提升性能。本文将这些单任务成功统一到一个可泛化的检索增强架构。

**通用架构**：BERT、GPT-2、BART、T5 等预训练模型在多任务上表现优异，但不包含检索模块。本文通过引入检索模块扩展了统一架构的能力边界。

**可学习检索**：大量工作研究基于神经检索器优化下游任务（如 QA、检索式阅读），包括基于检索监督、强化学习或潜变量优化等。本文展示单一检索增强架构可在多任务上微调并取得强性能。

**记忆架构**：文档索引可视为外部记忆，与记忆网络类似。相比嵌入式记忆，本文的记忆由原始文本组成，便于人类阅读与更新。该思路也被用于知识密集型对话。

**Retrieve-and-Edit**：与 retrieve-and-edit 方法类似，但本文强调从多个检索文档聚合内容与潜变量检索，而非对单条检索样本轻量编辑。

## 6 讨论
本文提出了同时访问参数化与非参数化记忆的生成模型，在开放域问答上达到最优，并且在人类评估中被认为更真实、更具体。我们验证了学习到的检索模块有效性，并展示了通过替换索引更新模型知识。未来可探索将检索器与生成器联合预训练（如类似 BART 的去噪目标），并进一步研究两类记忆的交互与融合方式。

## Broader Impact（更广泛影响）
RAG 更强的事实依托与可解释性可用于医疗等场景，为社会提供益处；但外部知识源可能带偏差，且生成模型可被滥用于虚假内容、冒充与垃圾信息生成。语言模型自动化也可能影响就业。为缓解风险，可用 AI 系统反制错误信息与自动化滥用。

## 致谢
感谢审稿人的建设性意见，以及 HuggingFace 在开源方面的支持。感谢 Kyunghyun Cho 与 Sewon Min 的讨论建议。EP 获 NSF Graduate Research Fellowship 支持；PL 受 FAIR PhD 项目资助。

## 附录
### A 实现细节
开放域 QA：RAG-Token 使用 15 个检索文档，RAG-Sequence 使用 50 个文档并采用 Thorough Decoding。QA 采用贪心解码。Open-MSMarco 与 Jeopardy 任务使用 10 个检索文档，beam size=4，RAG-Sequence 使用 Fast Decoding。

### B 人类评估
图 4 展示人工评估界面。为避免位置偏差，A/B 句子随机对应模型。允许评估者联网查证，并给出详细说明与示例。加入金标句子评估评估者质量，两名评估者在金标上表现不佳，被移除。

### C 训练设置
使用 Fairseq 实现 RAG 与 BART；混合精度训练，8 张 32GB V100（也可单卡）。FAISS 在 CPU 上检索足够快，原始索引约需 100GB 内存，后续压缩至约 36GB。代码已迁移至 HuggingFace Transformers 并开源。

### D 开放域 QA 进一步细节
对 NQ 与 WQ 使用多答案标注分别训练，提升准确率。对 TriviaQA 过滤不适合作为训练目标的答案（如 emoji/拼写变体）。CuratedTrec 的正则答案通过检索 1000 文档匹配频次确定监督目标，如无匹配则用启发式生成。TriviaQA 同时报告 Web Dev 与 Wiki 测试集结果，后者更易在维基中回答。

### E FEVER 进一步细节
FEVER 分类遵循 [32] 做法：先重新生成 claim，再用最终隐状态表示进行分类，并在文档上边缘化。由于 FEVER 使用不同的维基版本，证据抽取子任务未直接处理，后续工作可进一步探索。

### F 空文档概率
尝试引入“空文档”机制以处理无可检索信息的情况（学习空文档向量、静态偏置或预测网络），但未显著提升性能，因此未采用。在 Open-MSMarco 中模型倾向对不需检索的问题检索固定文档集合，说明空文档机制可能非必要。

### G 参数规模
RAG 使用 DPR 的查询与文档编码器（各 110M 参数，文档编码器固定不训练）与 BART-large（406M），总可训练参数约 626M。T5-11B 虽更大但在 NQ 上显著低于 RAG-Sequence。非参数索引包含 2100 万个 728 维向量，可用 8-bit 存储以降低内存。

### H 检索坍塌
在部分任务（如故事生成）中检索模块可能“坍塌”为检索同一文档集合，生成器转而忽略检索文档，效果接近 BART。原因可能是任务对事实检索依赖弱或序列过长导致检索梯度不充分。

### I 数据规模
各数据集的训练/验证/测试样本数见原文表 7。

## 参考文献（原文）
参考文献列表与原文一致，略。
