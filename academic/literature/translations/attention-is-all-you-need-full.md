# Attention Is All You Need（全文翻译）

## 版权说明
在正确署名的前提下，Google 允许仅为新闻或学术用途转载本文中的表格与图。

## 题名
Attention Is All You Need

## 作者
Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Łukasz Kaiser、Illia Polosukhin

## 摘要
主流的序列转导模型通常基于包含编码器与解码器的复杂循环或卷积神经网络。性能最好的模型往往还通过注意力机制连接编码器与解码器。我们提出了一种新的简单网络架构——Transformer，它完全基于注意力机制构建，彻底去除了循环与卷积。我们在两个机器翻译任务上的实验表明，该模型在翻译质量上更优，同时更易并行化且训练时间显著更短。在 WMT 2014 英译德任务上，我们的模型取得 28.4 BLEU，相比已有最佳结果（包括集成模型）提高超过 2 BLEU。在 WMT 2014 英译法任务上，我们的模型以单模型达到 41.8 BLEU，使用 8 块 GPU 训练 3.5 天即可完成，训练成本仅为此前最佳模型的一小部分。我们还将该模型应用于英语成分句法分析任务，结果表明其在大规模与小规模训练数据上均具有良好的泛化能力。

## 贡献说明
作者贡献均等，作者顺序随机。Jakob 提出以自注意力替代 RNN 并启动评估；Ashish 与 Illia 设计并实现了首个 Transformer，并参与了几乎所有工作；Noam 提出了缩放点积注意力、多头注意力与无参数位置表示；Niki 在原始代码与 tensor2tensor 中设计、实现、调参与评估大量模型变体；Llion 负责初始代码库、高效推理与可视化，并实验新变体；Lukasz 与 Aidan 在 tensor2tensor 的设计实现中投入大量工作，替换早期代码并大幅提升性能。

工作说明：Aidan 在 Google Brain 实习期间完成工作；Illia 在 Google Research 实习期间完成工作。

会议：第 31 届神经信息处理系统大会（NIPS 2017），美国加州长滩。

## 1 引言
循环神经网络（RNN），尤其是 LSTM 与门控循环网络，已在语言建模与机器翻译等序列建模与转导任务中确立为最先进方法。大量研究继续推动循环语言模型与编码器—解码器结构的发展。

循环模型通常沿序列位置展开计算，隐藏状态 $h_t$ 由上一时刻 $h_{t-1}$ 与当前位置输入决定。这种固有的顺序性使得同一训练样本内部难以并行，随着序列长度增长，该限制更为突出。虽然已有工作通过因子分解与条件计算提高效率并在一定程度上提升性能，但顺序计算的根本限制仍然存在。

注意力机制已成为序列建模与转导中的关键组件，能够在不考虑距离的情况下建模输入或输出序列的依赖关系。但除少数例外，注意力机制通常与循环网络结合使用。

本文提出 Transformer：一种完全去除循环、完全依赖注意力机制以捕获全局依赖的模型架构。Transformer 能显著提高并行度，并在 8 块 P100 GPU 上训练 12 小时后达到新的翻译质量最优结果。

## 2 背景
减少顺序计算同样是 Extended Neural GPU、ByteNet 与 ConvS2S 等模型的动机，这些模型使用卷积网络作为基本构件，并在所有位置并行计算隐藏表示。在这些模型中，关联两个位置所需的操作次数随距离增长：ConvS2S 为线性增长，ByteNet 为对数增长，使得学习远距离依赖更困难。Transformer 将其缩减为常数级操作，但代价是注意力加权平均会降低有效分辨率，我们通过多头注意力进行补偿（见 3.2 节）。

自注意力（也称 intra-attention）用于在同一序列内建模不同位置间的依赖关系，已在阅读理解、摘要、文本蕴含与通用句子表示学习等任务中取得成功。

端到端记忆网络以循环注意力替代序列对齐的循环结构，在简单语言问答与语言建模任务中表现良好。

据我们所知，Transformer 是第一个完全依赖自注意力、而不使用序列对齐的 RNN 或卷积来表示输入与输出的转导模型。接下来我们将详细描述 Transformer、阐述自注意力的动机，并讨论其相对优势。

## 3 模型结构
大多数强竞争力的序列转导模型采用编码器—解码器结构。编码器将输入符号序列 $(x_1,\ldots,x_n)$ 映射为连续表示序列 $z=(z_1,\ldots,z_n)$；解码器基于 $z$ 自回归地产生输出符号序列 $(y_1,\ldots,y_m)$，并在每步使用已生成的符号作为额外输入。

Transformer 遵循这一总体结构，但在编码器与解码器两侧均采用堆叠的自注意力与逐位置前馈层（见图 1）。

### 3.1 编码器与解码器堆叠
**编码器**：由 $N=6$ 个相同层组成，每层包含两个子层：多头自注意力子层与逐位置前馈网络子层。每个子层外均有残差连接与层归一化：

$\text{LayerNorm}(x + \text{Sublayer}(x))$。

所有子层与嵌入层的输出维度均为 $d_{model}=512$。

**解码器**：同样由 $N=6$ 个相同层组成。除与编码器相同的两个子层外，解码器多一个编码器—解码器注意力子层，使其能够关注编码器输出。解码器自注意力子层使用掩码以阻止当前位置看到未来位置，结合输出嵌入的位移，保证自回归性质。

### 3.2 注意力
注意力函数可视为将查询（query）与一组键值对（key-value）映射到输出。查询、键、值与输出均为向量。输出为值的加权和，权重由查询与对应键的相容性函数给出。

#### 3.2.1 缩放点积注意力
我们采用“缩放点积注意力”（见图 2）。输入包括维度为 $d_k$ 的查询与键，以及维度为 $d_v$ 的值。我们计算查询与键的点积并除以 $\sqrt{d_k}$，随后对每个查询的结果进行 softmax 得到权重，再与值相乘：

$$
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \tag{1}
$$

常见注意力函数包括加性注意力与点积注意力。加性注意力使用带单隐层的前馈网络计算相容性；点积注意力与本文方法相同但不带缩放因子。两者复杂度相近，但点积注意力在实践中更快且更节省空间，因为可由高度优化的矩阵乘法实现。

当 $d_k$ 较大时，不缩放的点积可能过大，使 softmax 进入梯度极小区域。为此我们使用 $1/\sqrt{d_k}$ 进行缩放。

#### 3.2.2 多头注意力
我们不是只做一次 $d_{model}$ 维注意力，而是将查询、键、值分别线性投影为 $h$ 组子空间（维度为 $d_k,d_k,d_v$），并行计算注意力后拼接，再进行一次线性变换：

$$
\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,\ldots,\text{head}_h)W^O
$$

其中

$$
\text{head}_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)
$$

投影矩阵为 $W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$、$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$、$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$，$W^O\in\mathbb{R}^{hd_v\times d_{model}}$。本文使用 $h=8$，且 $d_k=d_v=d_{model}/h=64$，因此计算量与单头全维注意力相近。

多头注意力使模型能在不同位置与不同表示子空间上联合关注信息；单头注意力容易被平均化效应限制。

#### 3.2.3 注意力在模型中的三种用法
Transformer 在三处使用多头注意力：

- **编码器—解码器注意力**：查询来自解码器上一层，键/值来自编码器输出，使解码器可关注输入序列所有位置。
- **编码器自注意力**：查询/键/值都来自编码器上一层输出，使编码器中每个位置能关注前一层的所有位置。
- **解码器自注意力**：解码器位置只能关注当前位置及之前位置，通过对未来位置进行掩码实现。

### 3.3 逐位置前馈网络
每个编码器与解码器层还包含一个对每个位置独立且共享参数的前馈网络，由两层线性变换与 ReLU 构成：

$$
\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2 \tag{2}
$$

输入与输出维度为 $d_{model}=512$，中间层维度为 $d_{ff}=2048$。可视为核大小为 1 的卷积。

### 3.4 词嵌入与 Softmax
我们使用学习到的词嵌入将输入/输出 token 映射到 $d_{model}$ 维向量；解码器输出通过线性变换与 softmax 得到下一词概率。两处嵌入与预 softmax 线性变换共享权重矩阵，并在嵌入层将其乘以 $\sqrt{d_{model}}$。

### 3.5 位置编码
由于模型不含循环与卷积，需要注入位置信息。我们将位置编码与词嵌入相加。位置编码与嵌入同维度，便于相加。本文采用正弦/余弦位置编码：

$$
PE(pos,2i)=\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right),\quad
PE(pos,2i+1)=\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

该形式使模型可通过线性变换表示相对位置。我们也尝试了学习型位置嵌入，结果与正弦形式相近，但正弦形式可能更利于长度外推。

### 表 1：不同层类型的复杂度与路径长度
| 层类型 | 单层复杂度 | 最小顺序操作数 | 最长路径长度 |
| --- | --- | --- | --- |
| 自注意力 | $O(n^2\cdot d)$ | $O(1)$ | $O(1)$ |
| 循环 | $O(n\cdot d^2)$ | $O(n)$ | $O(n)$ |
| 卷积 | $O(k\cdot n\cdot d^2)$ | $O(1)$ | $O(\log_k(n))$ |
| 限制范围自注意力 | $O(r\cdot n\cdot d)$ | $O(1)$ | $O(n/r)$ |

## 4 为什么使用自注意力
本节从三个方面比较自注意力与常见的循环/卷积层：单层计算复杂度、可并行化程度（最小顺序操作数）以及长程依赖的路径长度。学习长程依赖的难点之一在于信号在网络中传播路径越长越难学习，因此路径长度越短越有利。

如表 1 所示，自注意力在常数级顺序操作内即可连接所有位置，而循环层需要 $O(n)$ 次顺序操作。对句子级任务而言，序列长度 $n$ 常小于表示维度 $d$，此时自注意力在计算复杂度上优于循环层。对超长序列，可限制每个位置仅关注邻域大小为 $r$ 的范围，以降低计算量，但路径长度将增至 $O(n/r)$。

卷积层若核宽 $k<n$，则不能一次性连接所有输入与输出位置，需要堆叠 $O(n/k)$ 层（连续卷积）或 $O(\log_k(n))$ 层（空洞卷积），从而增加最长路径长度。卷积的计算成本一般高于循环层一个 $k$ 因子。可分离卷积可将复杂度降至 $O(knd + nd^2)$，但即使 $k=n$，其复杂度也与“自注意力 + 逐位置前馈”的组合相当。

自注意力还带来更好的可解释性。我们在附录中展示注意力分布的例子，多个注意力头表现出与句法或语义结构相关的行为。

## 5 训练
本节描述模型训练设置。

### 5.1 训练数据与批处理
英德翻译使用 WMT 2014 英-德数据集，约 450 万句对，采用 BPE 子词编码，共享约 3.7 万词表。英法翻译使用 WMT 2014 英-法数据集，约 3600 万句对，采用 3.2 万词片段词表。训练时按序列长度近似分桶，每个 batch 含约 2.5 万源词与 2.5 万目标词。

### 5.2 硬件与训练时程
在 8 块 NVIDIA P100 GPU 上训练。基础模型每步约 0.4 秒，共 10 万步（约 12 小时）。大模型每步约 1.0 秒，共 30 万步（约 3.5 天）。

### 5.3 优化器
使用 Adam 优化器，$\beta_1=0.9$，$\beta_2=0.98$，$\epsilon=10^{-9}$。学习率调度为：

$$
lrate=d_{model}^{-0.5}\cdot\min(step\_num^{-0.5},\;step\_num\cdot warmup\_steps^{-1.5}) \tag{3}
$$

即前 $warmup\_steps$ 线性升高，之后按步数平方根反比衰减。本文使用 $warmup\_steps=4000$。

### 5.4 正则化
- **残差 Dropout**：对子层输出施加 dropout 后再与输入相加并归一化，同时对嵌入与位置编码之和进行 dropout。基础模型使用 $P_{drop}=0.1$。
- **标签平滑**：使用 $\epsilon_{ls}=0.1$ 的标签平滑，虽会降低困惑度，但可提升准确率与 BLEU。

### 表 2：英德与英法翻译 BLEU 与训练成本
表 2 比较了多种模型在 WMT 2014 英德与英法 newstest2014 测试集上的 BLEU 及训练成本（FLOPs）。由于 PDF 表格抽取会引起换行与列对齐偏移，以下为原文数值的顺序列表，完整表格格式请以原文为准。

- 模型顺序：ByteNet；Deep-Att + PosUnk；GNMT + RL；ConvS2S；MoE；Deep-Att + PosUnk Ensemble；GNMT + RL Ensemble；ConvS2S Ensemble；Transformer（base）；Transformer（big）。
- EN-DE BLEU（顺序列表）：23.75，24.6，25.16，26.03，26.30，26.36，27.3，28.4。
- EN-FR BLEU（顺序列表）：39.2，39.92，40.46，40.56，40.4，41.16，41.29，38.1，41.8。
- 训练成本 EN-DE（FLOPs，顺序列表）：$2.3\times10^{19}$，$9.6\times10^{18}$，$2.0\times10^{19}$，$1.8\times10^{20}$，$7.7\times10^{19}$，$1.0\times10^{20}$，$1.4\times10^{20}$，$1.5\times10^{20}$，$1.2\times10^{20}$，$8.0\times10^{20}$，$1.1\times10^{21}$，$1.2\times10^{21}$。
- 训练成本 EN-FR（FLOPs，顺序列表）：$3.3\times10^{18}$，$2.3\times10^{19}$。

## 6 实验结果

### 6.1 机器翻译
在 WMT 2014 英译德任务上，Transformer（big）相比此前最佳模型（含集成）提高超过 2 BLEU，达到 28.4。训练耗时 3.5 天（8 块 P100）。基础模型也优于此前所有模型与集成，且训练成本更低。

在英译法任务上，Transformer（big）达到 41.0 BLEU，优于所有单模型，训练成本不到此前最优模型的四分之一。该设置使用 $P_{drop}=0.1$（而非 0.3）。

基础模型采用最后 5 个检查点平均，大模型采用最后 20 个检查点平均。解码使用 beam size=4 与长度惩罚 $\alpha=0.6$，最大输出长度设为输入长度 + 50。

训练成本通过训练时间 × GPU 数 × GPU 单精度 FLOPs 估计（K80、K40、M40、P100 分别取 2.8、3.7、6.0、9.5 TFLOPS）。

### 6.2 模型变体
我们对基础模型进行多种变体实验（见表 3），评估组件的重要性。

- **(A)** 在计算量固定的情况下调整注意力头数与 $d_k,d_v$，单头注意力比最佳设置低 0.9 BLEU，头数过多也会降低性能。
- **(B)** 减小 $d_k$ 会显著降低性能，说明简单点积的相容性函数对维度敏感。
- **(C)(D)** 更大的模型带来更好性能，dropout 有效抑制过拟合。
- **(E)** 学习型位置嵌入与正弦位置编码效果几乎一致。

表 3 数值较多，此处省略，详见原文表格。

### 6.3 英语成分句法分析
为验证 Transformer 的任务泛化能力，我们在英语成分句法分析任务上进行实验。该任务输出结构约束强，且输出长度显著长于输入，RNN 序列模型在小数据条件下难以达到最优结果。

我们使用 4 层 Transformer（$d_{model}=1024$）在 Penn Treebank 的 WSJ 部分训练（约 4 万句），并在半监督设置下使用约 1700 万句的高置信语料。仅对 dropout、学习率与 beam size 进行少量调参。推理时最大输出长度设为输入长度 + 300，beam size=21，$\alpha=0.3$。

结果如表 4 所示，Transformer 在不做任务特定优化的情况下仍表现良好，仅略低于 RNN Grammar 模型；并在 WSJ-only 训练条件下超过 BerkeleyParser。

### 表 4：英语成分句法分析（WSJ 23 F1）
| 模型 | 训练设置 | F1 |
| --- | --- | --- |
| Vinyals & Kaiser et al. (2014) [37] | WSJ only, discriminative | 88.3 |
| Petrov et al. (2006) [29] | WSJ only, discriminative | 90.4 |
| Zhu et al. (2013) [40] | WSJ only, discriminative | 90.4 |
| Dyer et al. (2016) [8] | WSJ only, discriminative | 91.7 |
| Transformer (4 layers) | WSJ only, discriminative | 91.3 |
| Zhu et al. (2013) [40] | semi-supervised | 91.3 |
| Huang & Harper (2009) [14] | semi-supervised | 91.3 |
| McClosky et al. (2006) [26] | semi-supervised | 92.1 |
| Vinyals & Kaiser et al. (2014) [37] | semi-supervised | 92.1 |
| Transformer (4 layers) | semi-supervised | 92.7 |
| Luong et al. (2015) [23] | multi-task | 93.0 |
| Dyer et al. (2016) [8] | generative | 93.3 |

## 7 结论
本文提出 Transformer：一种完全基于注意力机制的序列转导模型，用多头自注意力替代编码器—解码器结构中的循环层。该模型在机器翻译任务上训练速度更快、性能更优，并在英德与英法翻译上达到新的最优结果。Transformer 也能良好泛化到英语句法分析任务。

我们计划将 Transformer 拓展到文本以外的输入/输出模态（如图像、音频与视频），并研究局部/受限注意力以高效处理超长序列。

代码地址：https://github.com/tensorflow/tensor2tensor

## 致谢
感谢 Nal Kalchbrenner 与 Stephan Gouws 的宝贵意见、纠错与启发。

## 附录：注意力可视化
- 图 3：编码器第 5 层自注意力对长距离依赖的示例，多个注意力头关注动词“making”的远距离依赖，形成“making...more difficult”的结构。
- 图 4：编码器第 5 层两个注意力头在指代消解中的表现，特别是对“its”的注意力分布。
- 图 5：不同注意力头体现不同句法/语义行为的示例。

注：原文中包含对应图示与输入序列，本译文为文字说明。

## 参考文献（原文）
参考文献列表与原文一致，略。
