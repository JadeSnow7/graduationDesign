\chapter{方法与实现}
\section{需求分析与总体设计}
平台面向管理员、教师/助教与学生三类主要角色，核心需求可归纳为三条主线：其一是\textbf{过程性数据沉淀}，能够围绕写作提交、修改与对话辅导记录学习事件，并形成长期可追踪的学生画像；其二是\textbf{写作类型感知的反馈}，针对文献综述、课程论文、学位论文与摘要等不同类型提供差异化 rubric 与结构化建议；其三是\textbf{可控与可追溯}，在大模型生成建议时提供证据引用与复核入口，降低幻觉带来的教学风险。

非功能需求方面，系统需要具备可扩展与可维护的工程结构（便于迭代模型与课程模块）、清晰的权限边界（避免越权访问与答案泄露）、以及跨终端一致的调用契约（避免 Web/Mobile 接口分叉）。基于上述需求，本文采用前后端分离与服务化架构，将教学业务、AI 能力与检索索引解耦，通过统一鉴权与模块门控策略保证能力可治理。

\section{系统架构}
系统整体采用“客户端—后端业务—AI 服务—检索/存储”的分层结构：客户端提供 Web 与移动端入口；后端提供业务 API、JWT 鉴权、RBAC 权限与课程模块门控；AI 服务负责提示模板、写作分析与对话能力的编排，并通过 OpenAI-compatible 接口调用上游大模型推理服务；GraphRAG 作为可选组件提供课程知识库检索与引用溯源。整体架构如图~\ref{fig:architecture} 所示。

\begin{figure}[htbp]
  \centering
  \fbox{\rule{0pt}{5cm}\rule{10cm}{0pt}}
  \caption{平台总体架构示意（占位）}
  \label{fig:architecture}
\end{figure}

\section{统一契约与跨端共享}
为避免跨端开发中出现“同一业务多套接口/字段”的问题，系统采用 Monorepo 组织代码，并抽取共享包沉淀 types 与统一 SDK。共享 SDK 负责：
(1) 统一请求层（鉴权头、错误归一、超时与重试策略）；
(2) 以类型定义约束前后端契约，减少字段不一致导致的运行时错误；
(3) 为 Web/Mobile 提供一致的 API 调用方式，使平台差异主要集中在 UI 与交互层。
该设计降低了多端协作成本，并为后续在不同课程模块间复用能力提供基础。

\section{学生中心数据模型与学习事件}
平台以学生为中心组织数据。在课程层面，写作提交被建模为可追踪的业务对象：包含写作类型、标题与内容、提交时间、AI 分析结果与教师反馈等字段；在过程层面，系统记录关键学习事件（如写作提交、写作分析完成、对话辅导、学习时长心跳等），用于后续聚合形成学生画像。画像不仅包含“分数”，更强调可解释的能力维度（例如结构清晰度、证据使用、学术语气与引用规范），从而支持纵向对比与个性化干预。

\section{引导式学习与薄弱点追踪}
除“写作提交—分析—反馈”流程外，平台提供面向过程性辅导的引导式学习能力（guided learning）：系统首先为某一学习主题生成 3--6 步的学习路径（learning path），随后以苏格拉底式提问引导学生逐步完成每一步。例如，在写作课程中，学习主题可围绕 thesis statement、段落结构、证据使用与引用规范等展开，系统会在每轮对话中只提出一个关键问题，并根据学生回答的完整性决定是否进入下一步，从而把复杂能力训练拆解为可管理的阶段。

实现上，AI 服务提供 \texttt{/v1/chat/guided} 端点，使用会话状态（session\_id）维护学习目标、当前步骤与路径结构，并在首轮由模型输出 JSON 路径以便前端渲染进度。为将对话信号沉淀为画像特征，系统在每次对话后对助教回复中的纠错与提示语句做轻量检测，提取与写作相关的薄弱点概念（如“逻辑连接”“引用规范”“论点展开”），记录到会话中并可同步到后端学习档案。结合 GraphRAG 时，系统会把检索到的课程规范与示例片段作为证据注入对话上下文，要求回答标注引用编号并在证据不足时追问或拒答，从而提升引导式建议的可追溯性与可复核性。

\section{写作类型感知的智能分析服务}
写作分析服务以“写作类型 + rubric + 结构化输出”为核心。服务端首先识别或校验写作类型，并选择对应的评估维度与权重；随后调用上游大模型生成反馈，并将输出解析为维度评分、优点与改进建议等结构化字段，便于前端展示与教师复核。与通用润色工具不同，本文更关注“可执行建议”：例如指出段落功能缺失、论证链条不完整、证据不足或引用格式问题，并给出可操作的修改方案。该设计使反馈更贴近课程要求，也更便于后续沉淀高质量标注数据。

\section{GraphRAG 知识库与检索增强生成}
为降低大模型在写作辅导中的幻觉风险，系统引入 GraphRAG 检索增强生成流程\cite{lewis2020}。针对研究生《学术规范与论文写作》课程中常见的“引用格式错误”与“学术不端风险”，本文构建了基于向量检索的课程知识库。构建过程如下：首先，将课程讲义、学校学位论文写作规范及优秀的历年范文进行结构化清洗，并按照“章节—段落”的层级进行切分（Chunking），默认切片大小设定为 1200 字符。其次，采用 `text-embedding-v3` 模型将切分后的文本片段转化为高维度向量（Embedding），并存储于 FAISS 向量数据库中。
当学生在对话中咨询关于引用规范或格式要求的问题时，系统首先将用户查询转化为向量，通过余弦相似度在向量数据库中检索最相关的 $K$ 个规范条款或范文片段。检索到的片段被作为“证据（Evidence）”注入到大模型的上下文提示词（Prompt）中，并强制要求模型仅依据检索到的证据回答，并在回答末尾标注引用来源编号。这一“向量化—检索—注入—生成”的闭环不仅显著降低了模型的幻觉风险，更模拟了真实的学术问题解决过程——即“查阅规范—理解条款—应用执行”，从而在技术实现的底层逻辑上契合了课程的教学目标。

\subsection{学术规范向量知识库构建过程}
针对《学术规范与论文写作》课程中“建议容易泛化、缺少课程依据”的问题，系统将课程规范资料与论文写作指南按统一流程构建为向量知识库。首先，离线 ingestion 支持 `.md/.markdown/.pdf/.txt` 多源文本输入；随后按章节与段落进行切分，并以 `--chunk-chars=1200` 作为默认分块上限，将原始文本转换为可检索的知识片段。接着，系统对片段执行 Embedding 向量化（`api|local|hash|env`，默认模型 `text-embedding-v3`），并将向量写入 `FAISS` 向量存储，同时维护图索引中的节点与邻接关系以支持图扩展检索。

在线推理阶段，查询先经过语义检索与关键词/图扩展召回，再将命中的证据片段注入提示词上下文，约束模型在证据范围内生成并标注引用编号。该“原文资料 $\rightarrow$ chunks $\rightarrow$ embeddings $\rightarrow$ vector store $\rightarrow$ 检索注入生成”的链路，将回答从“语言模型先验”转为“课程证据驱动”，可显著降低《学术规范与论文写作》辅导中的幻觉建议与领域知识缺失问题，并提升教师复核与过程追溯的可行性。

\section{工具调用与可验证能力}
除写作建议外，教学场景中仍存在需要“可验证计算/查询”的任务，例如对字数、结构要素或格式规则进行检查，或在理工类课程中进行数值计算与仿真。为此，AI 服务提供工具调用接口，使模型在需要精确结果时可调用外部工具并将结果回注到对话中，再生成解释性回答。工具调用能力本质上为系统提供了“外部可验证执行器”，用于约束模型的自由生成范围，降低“凭空计算/编造规则”的风险。本文在原型中实现了基础工具集合，并预留面向写作场景的扩展空间（如引用格式校验、结构要素检查等）。

\section{模型后训练与评测管线}
为使模型更贴近“端侧低延迟 + 场景化决策”目标，本文在原有训练工具链基础上，于 2026-02-10 完成了端侧课程助手的本机微调实验。该实验采用 \texttt{ms-swift} 作为统一训练与部署框架，围绕课程资源检索、学习追踪、简单问答与复杂推理分流提示四类任务，构建“数据转换--训练--部署--联测”闭环。

\subsection{数据转换与质量校验}
原始样本采用 \texttt{instruction/input/output} 结构。为适配 \texttt{ms-swift} 的对话式监督微调，本文将数据统一转换为 \texttt{messages} 格式（system/user/assistant），并在 system 提示中固化端侧行为约束：本地优先、结构化回答、复杂推理返回转云端提示文案。转换后的数据统计如表~\ref{tab:edge-split-stats} 与表~\ref{tab:edge-task-stats} 所示。

\begin{table}[htbp]
  \centering
  \caption{端侧微调数据集划分（2026-02-10）}
  \label{tab:edge-split-stats}
  \begin{tabular}{lrl}
    \toprule
    数据集 & 样本数 & 用途 \\
    \midrule
    train & 400 & LoRA 训练 \\
    valid & 50 & 训练中评估 \\
    test & 50 & 联测抽样 \\
    总计 & 500 & 解析成功率 100\% \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{端侧微调任务类型分布}
  \label{tab:edge-task-stats}
  \begin{tabular}{lr}
    \toprule
    任务类型 & 样本数 \\
    \midrule
    course\_resource & 200 \\
    learning\_tracking & 150 \\
    simple\_qa & 100 \\
    complex\_reasoning & 50 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{本机 LoRA 微调配置}
本轮实验固定采用 LoRA（Apple MPS）而非 QLoRA，以规避本机环境下 bitsandbytes 的 GPU 能力限制。核心训练参数如表~\ref{tab:edge-train-config} 所示。

\begin{table}[htbp]
  \centering
  \caption{端侧本机 LoRA 训练配置（ms-swift）}
  \label{tab:edge-train-config}
  \begin{tabular}{lp{8.5cm}}
    \toprule
    配置项 & 取值 \\
    \midrule
    训练框架 & ms-swift 3.12.4 \\
    训练模式 & LoRA（Apple MPS，本机） \\
    基座模型 & Qwen3-0.6B-Instruct（HF 本地路径） \\
    model\_type & qwen3\_nothinking \\
    LoRA 参数 & rank=8, alpha=16, dropout=0.05 \\
    target\_modules & q\_proj, v\_proj \\
    learning\_rate & $2 \times 10^{-4}$ \\
    num\_train\_epochs & 3 \\
    per\_device\_train\_batch\_size & 1 \\
    gradient\_accumulation\_steps & 8 \\
    max\_length & 512 \\
    logging/save/eval steps & 10 / 50 / 50 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{训练结果与最佳检查点选择}
训练日志显示 loss 持续下降，并在第 100 step 达到最优验证指标。关键过程数据如表~\ref{tab:edge-train-metrics} 所示。

\begin{table}[htbp]
  \centering
  \caption{端侧 LoRA 训练关键指标（2026-02-10）}
  \label{tab:edge-train-metrics}
  \begin{tabular}{lccc}
    \toprule
    训练节点 & train loss & eval loss & eval token\_acc \\
    \midrule
    step 50 & 0.2503 & 0.2368 & 0.9529 \\
    step 100 & 0.0334 & \textbf{0.0245} & \textbf{0.9957} \\
    step 150 & 0.0170 & - & - \\
    \bottomrule
  \end{tabular}
\end{table}

综合 \texttt{trainer\_state} 与日志记录，最终选择 \texttt{checkpoint-100} 作为部署产物（best\_model\_checkpoint）。训练尾段虽出现本机磁盘临时空间不足告警，但不影响最优 checkpoint 的落盘与后续部署验证。

\subsection{部署与客户端联测}
本文将 \texttt{checkpoint-100} 通过 \texttt{swift deploy} 发布为 OpenAI-compatible 本地服务（\texttt{127.0.0.1:18080}），并按“AI Service $\rightarrow$ Backend $\rightarrow$ Expo Web”链路完成联测。系统路由策略固定为 \texttt{local\_first}，非生产环境云端 fallback 关闭。服务健康检查 \texttt{GET /health}、\texttt{GET /v1/models} 与 \texttt{POST /v1/chat/completions} 均返回成功。

\begin{table}[htbp]
  \centering
  \caption{端侧模型客户端联测结果（12 条固定用例）}
  \label{tab:edge-e2e-results}
  \begin{tabular}{lccc}
    \toprule
    用例类型 & 条数 & 通过标准 & 结果 \\
    \midrule
    课程资源检索 & 4 & 非空回复 & 4/4 \\
    学习追踪 & 3 & 非空回复 & 3/3 \\
    简单问答 & 3 & 非空回复 & 3/3 \\
    复杂推理提示 & 2 & 返回“转发云端”语义 & 2/2 \\
    汇总 & 12 & 非空回复成功率 $\geq 90\%$ & \textbf{100\%（PASS）} \\
    \bottomrule
  \end{tabular}
\end{table}

上述结果表明，本轮端侧训练/微调链路已实现工程可复现，且在不改动后端公共 API（\texttt{/api/v1/ai/chat}）前提下完成了业务接入验证。ONNX 导出、ANE/NPU 编译优化、INT8 量化后联合评测与端侧功耗 profiling 将在后续阶段进行。

\section{系统原型实现与企业微信集成}
平台前端采用 React + TypeScript 实现 Web 客户端，并提供基于 Expo 的移动端实现；后端采用 Go/Gin 提供课程、写作与学习事件相关 API，并通过 JWT 与 RBAC 实现权限治理；AI 服务基于 FastAPI，实现对话、写作分析、GraphRAG 与工具调用等能力，并对接 OpenAI-compatible 上游推理服务。原型部署层面提供 Docker Compose 配置以便快速启动与验证；对于企业微信等场景，系统预留 OAuth 与组织对接能力，以支持后续在真实教学流程中落地。

\section{系统测试与评估}
系统测试与评估围绕功能正确性、可追溯性与工程稳定性展开。功能测试验证写作提交与分析流程、权限校验、学习事件记录与查询等关键链路；可追溯性测试关注 RAG 模式下的引用一致性与“证据不足时追问/拒答”行为；工程测试关注典型请求的响应时间与服务稳定性。对于模型效果评估，本文采用“固定回归集 + 案例分析”的方式进行离线对比，并预留进一步的用户试用与课堂验证方案，用于在真实课程中评估建议的可采纳性与对学习效果的影响。
