\chapter{方法与实现}
\section{需求分析与总体设计}
平台面向管理员、教师/助教与学生三类主要角色，核心需求可归纳为三条主线：其一是\textbf{过程性数据沉淀}，能够围绕写作提交、修改与对话辅导记录学习事件，并形成长期可追踪的学生画像；其二是\textbf{写作类型感知的反馈}，针对文献综述、课程论文、学位论文与摘要等不同类型提供差异化 rubric 与结构化建议；其三是\textbf{可控与可追溯}，在大模型生成建议时提供证据引用与复核入口，降低幻觉带来的教学风险。

非功能需求方面，系统需要具备可扩展与可维护的工程结构（便于迭代模型与课程模块）、清晰的权限边界（避免越权访问与答案泄露）、以及跨终端一致的调用契约（避免 Web/Mobile 接口分叉）。基于上述需求，本文采用前后端分离与服务化架构，将教学业务、AI 能力与检索索引解耦，通过统一鉴权与模块门控策略保证能力可治理。

\section{系统架构}
系统整体采用“客户端—后端业务—AI 服务—检索/存储”的分层结构：客户端提供 Web 与移动端入口；后端提供业务 API、JWT 鉴权、RBAC 权限与课程模块门控；AI 服务负责提示模板、写作分析与对话能力的编排，并通过 OpenAI-compatible 接口调用上游大模型推理服务；GraphRAG 作为可选组件提供课程知识库检索与引用溯源。整体架构如图~\ref{fig:architecture} 所示。

\begin{figure}[htbp]
  \centering
  \fbox{\rule{0pt}{5cm}\rule{10cm}{0pt}}
  \caption{平台总体架构示意（占位）}
  \label{fig:architecture}
\end{figure}

\section{统一契约与跨端共享}
为避免跨端开发中出现“同一业务多套接口/字段”的问题，系统采用 Monorepo 组织代码，并抽取共享包沉淀 types 与统一 SDK。共享 SDK 负责：
(1) 统一请求层（鉴权头、错误归一、超时与重试策略）；
(2) 以类型定义约束前后端契约，减少字段不一致导致的运行时错误；
(3) 为 Web/Mobile 提供一致的 API 调用方式，使平台差异主要集中在 UI 与交互层。
该设计降低了多端协作成本，并为后续在不同课程模块间复用能力提供基础。

\section{学生中心数据模型与学习事件}
平台以学生为中心组织数据。在课程层面，写作提交被建模为可追踪的业务对象：包含写作类型、标题与内容、提交时间、AI 分析结果与教师反馈等字段；在过程层面，系统记录关键学习事件（如写作提交、写作分析完成、对话辅导、学习时长心跳等），用于后续聚合形成学生画像。画像不仅包含“分数”，更强调可解释的能力维度（例如结构清晰度、证据使用、学术语气与引用规范），从而支持纵向对比与个性化干预。

\section{引导式学习与薄弱点追踪}
除“写作提交—分析—反馈”流程外，平台提供面向过程性辅导的引导式学习能力（guided learning）：系统首先为某一学习主题生成 3--6 步的学习路径（learning path），随后以苏格拉底式提问引导学生逐步完成每一步。例如，在写作课程中，学习主题可围绕 thesis statement、段落结构、证据使用与引用规范等展开，系统会在每轮对话中只提出一个关键问题，并根据学生回答的完整性决定是否进入下一步，从而把复杂能力训练拆解为可管理的阶段。

实现上，AI 服务提供 \texttt{/v1/chat/guided} 端点，使用会话状态（session\_id）维护学习目标、当前步骤与路径结构，并在首轮由模型输出 JSON 路径以便前端渲染进度。为将对话信号沉淀为画像特征，系统在每次对话后对助教回复中的纠错与提示语句做轻量检测，提取与写作相关的薄弱点概念（如“逻辑连接”“引用规范”“论点展开”），记录到会话中并可同步到后端学习档案。结合 GraphRAG 时，系统会把检索到的课程规范与示例片段作为证据注入对话上下文，要求回答标注引用编号并在证据不足时追问或拒答，从而提升引导式建议的可追溯性与可复核性。

\section{写作类型感知的智能分析服务}
写作分析服务以“写作类型 + rubric + 结构化输出”为核心。服务端首先识别或校验写作类型，并选择对应的评估维度与权重；随后调用上游大模型生成反馈，并将输出解析为维度评分、优点与改进建议等结构化字段，便于前端展示与教师复核。与通用润色工具不同，本文更关注“可执行建议”：例如指出段落功能缺失、论证链条不完整、证据不足或引用格式问题，并给出可操作的修改方案。该设计使反馈更贴近课程要求，也更便于后续沉淀高质量标注数据。

\section{GraphRAG 知识库与检索增强生成}
为降低大模型在写作辅导中的幻觉风险，系统引入 GraphRAG 检索增强生成流程\cite{lewis2020}。针对研究生《学术规范与论文写作》课程中常见的“引用格式错误”与“学术不端风险”，本文构建了基于向量检索的课程知识库。构建过程如下：首先，将课程讲义、学校学位论文写作规范及优秀的历年范文进行结构化清洗，并按照“章节—段落”的层级进行切分（Chunking），默认切片大小设定为 1200 字符。其次，采用 `text-embedding-v3` 模型将切分后的文本片段转化为高维度向量（Embedding），并存储于 FAISS 向量数据库中。
当学生在对话中咨询关于引用规范或格式要求的问题时，系统首先将用户查询转化为向量，通过余弦相似度在向量数据库中检索最相关的 $K$ 个规范条款或范文片段。检索到的片段被作为“证据（Evidence）”注入到大模型的上下文提示词（Prompt）中，并强制要求模型仅依据检索到的证据回答，并在回答末尾标注引用来源编号。这一“向量化—检索—注入—生成”的闭环不仅显著降低了模型的幻觉风险，更模拟了真实的学术问题解决过程——即“查阅规范—理解条款—应用执行”，从而在技术实现的底层逻辑上契合了课程的教学目标。

\subsection{学术规范向量知识库构建过程}
针对《学术规范与论文写作》课程中“建议容易泛化、缺少课程依据”的问题，系统将课程规范资料与论文写作指南按统一流程构建为向量知识库。首先，离线 ingestion 支持 `.md/.markdown/.pdf/.txt` 多源文本输入；随后按章节与段落进行切分，并以 `--chunk-chars=1200` 作为默认分块上限，将原始文本转换为可检索的知识片段。接着，系统对片段执行 Embedding 向量化（`api|local|hash|env`，默认模型 `text-embedding-v3`），并将向量写入 `FAISS` 向量存储，同时维护图索引中的节点与邻接关系以支持图扩展检索。

在线推理阶段，查询先经过语义检索与关键词/图扩展召回，再将命中的证据片段注入提示词上下文，约束模型在证据范围内生成并标注引用编号。该“原文资料 $\rightarrow$ chunks $\rightarrow$ embeddings $\rightarrow$ vector store $\rightarrow$ 检索注入生成”的链路，将回答从“语言模型先验”转为“课程证据驱动”，可显著降低《学术规范与论文写作》辅导中的幻觉建议与领域知识缺失问题，并提升教师复核与过程追溯的可行性。

\section{工具调用与可验证能力}
除写作建议外，教学场景中仍存在需要“可验证计算/查询”的任务，例如对字数、结构要素或格式规则进行检查，或在理工类课程中进行数值计算与仿真。为此，AI 服务提供工具调用接口，使模型在需要精确结果时可调用外部工具并将结果回注到对话中，再生成解释性回答。工具调用能力本质上为系统提供了“外部可验证执行器”，用于约束模型的自由生成范围，降低“凭空计算/编造规则”的风险。本文在原型中实现了基础工具集合，并预留面向写作场景的扩展空间（如引用格式校验、结构要素检查等）。

\section{模型后训练与评测管线}
为使模型更贴近课程风格与任务需求，本文实现了面向写作/对话数据的后训练管线：包括数据规范、数据准备脚本、LoRA/QLoRA 微调脚本\cite{hu2021lora,dettmers2023qlora}与离线评测脚本。训练数据以多轮对话 JSONL 表示，并区分 tool/rag/style 等样本类型；评测阶段以固定回归集输出指标与案例，辅助迭代数据与提示策略。受数据规模与时间限制，本文先使用小规模样例数据完成端到端验证：训练脚本可稳定产出 adapter，评测脚本可输出困惑度、格式一致性与拒答准确率等指标，为后续在 Qwen3 8B 上进行 100k 规模训练提供工程基础。

为降低“数据格式不一致导致训练失败”的工程风险，本文在训练前增加了数据蒸馏与冒烟验证步骤：将 chat-style 的训练/评测 JSONL 通过 \texttt{scripts/ai/distill\_data.py} 蒸馏为 prompt/response 格式，并用 \texttt{scripts/ai/train\_smoke.py} 在分钟级输出困惑度等轻量指标，用于验证数据链路与指标输出链路可复现。需要强调的是，smoke 指标仅用于证明训练与评测链路可用，并不代表最终模型效果。

\begin{table}[htbp]
  \centering
  \caption{样例训练链路验证结果（用于证明训练与评测链路可用）}
  \label{tab:smoke-train}
  \begin{tabular}{lccp{6.2cm}}
    \toprule
    指标 & 训练集 & 验证集 & 说明 \\
    \midrule
    样本数 & 3 & 2 & 小规模 JSONL 样例数据，仅用于链路验证 \\
    Token 数 & 68 & 50 & 以分词后 token 计 \\
    困惑度（PPL） & 32.95 & 41.30 & 使用轻量模型完成端到端训练与评测，数值不代表最终效果 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{阶段性训练结果同步（2026-02-08）}
在完成训练脚本与评测脚本的端到端连通验证后，项目于 2026-02-08 执行了首次 \texttt{all} 多任务训练评测（小样本）与随机三组回归测试，并将结果同步为统一事实源。指标如表~\ref{tab:stage-train-20260208} 所示。

\begin{table}[htbp]
  \centering
  \caption{阶段性训练结果（2026-02-08，同步批次）}
  \label{tab:stage-train-20260208}
  \begin{tabular}{lp{2.0cm}cccc}
    \toprule
    评测批次 & 样本规模 & key\_point\_coverage & refusal\_accuracy & response\_format & tool\_call\_accuracy \\
    \midrule
    首次 all 训练 & $n=5$ & 0.9167 & 0.8000 & 1.0000 & 0.0000 \\
    随机三组回归均值 & $3 \times n=6$ & 0.7333 & 0.7778 & 0.8333 & 0.0000 \\
    \bottomrule
  \end{tabular}
\end{table}

需要说明的是，上述结果仅用于证明“训练—评测—文档同步”链路可复现，属于阶段性验证数据，不作为本文正式实验结论。后续正式实验将在真实 \texttt{style/tool/rag} 数据闭环后重新训练并报告主结果。

\section{系统原型实现与企业微信集成}
平台前端采用 React + TypeScript 实现 Web 客户端，并提供基于 Expo 的移动端实现；后端采用 Go/Gin 提供课程、写作与学习事件相关 API，并通过 JWT 与 RBAC 实现权限治理；AI 服务基于 FastAPI，实现对话、写作分析、GraphRAG 与工具调用等能力，并对接 OpenAI-compatible 上游推理服务。原型部署层面提供 Docker Compose 配置以便快速启动与验证；对于企业微信等场景，系统预留 OAuth 与组织对接能力，以支持后续在真实教学流程中落地。

\section{系统测试与评估}
系统测试与评估围绕功能正确性、可追溯性与工程稳定性展开。功能测试验证写作提交与分析流程、权限校验、学习事件记录与查询等关键链路；可追溯性测试关注 RAG 模式下的引用一致性与“证据不足时追问/拒答”行为；工程测试关注典型请求的响应时间与服务稳定性。对于模型效果评估，本文采用“固定回归集 + 案例分析”的方式进行离线对比，并预留进一步的用户试用与课堂验证方案，用于在真实课程中评估建议的可采纳性与对学习效果的影响。
