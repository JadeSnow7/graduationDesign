\chapter{绪论}
\section{研究背景与意义}
研究生课程《学术规范与论文写作》中研究生阶段的专业学术写作是科研训练的重要组成部分，其难点不在于“写出一段文字”，而在于持续地完成选题定位、论证组织、证据使用与学术表达规范的综合训练。以研究生《学术规范与论文写作》课程为例，学生往往需要同时处理语法与措辞准确性、段落结构与逻辑衔接、引用与证据一致性等问题；不同学生在语言基础、科研经验与表达习惯上存在显著差异，使得“一次性评分”很难形成有效的改进闭环。写作教学高度依赖过程性反馈：从选题与大纲，到草稿迭代与修改记录，再到教师或助教的具体建议，这些信息共同构成学生能力发展的轨迹。如何把这些过程性信息系统化沉淀，并转化为可执行的个性化建议，是写作课程面临的核心问题之一。

近年来，Transformer 架构\cite{vaswani2017}推动大语言模型在自然语言理解与生成方面取得突破，对话式模型（如 ChatGPT）\cite{openai2022}在解释、改写与建议生成方面表现突出，为写作辅导提供了新的技术手段。然而，大模型在教育场景中仍面临两个突出挑战：其一是建议的可靠性与可追溯性不足，模型可能给出“看似合理但缺乏依据”的写作建议；其二是个性化不足，模型难以在缺少学习过程数据的情况下稳定地贴合学生水平与课程 rubric。检索增强生成（RAG）\cite{lewis2020}与知识图谱\cite{hogan2020}为上述问题提供了可行路径：通过将课程规范、范文片段与常见错误案例构建为可检索的证据库，把生成行为锚定在可引用的片段之上，从而降低幻觉并便于教师复核。

基于此，本文以《学术规范与论文写作》课程为文本任务试点，并以《电磁场与电磁波》课程作为工具型任务的配套验证基于此，本文以研究生《学术规范与论文写作》与本科生《电磁场与电磁波》为试点场景，提出并实现一套“以学生为中心”的智能教学平台：围绕学生学习过程建立档案与事件记录，结合写作类型感知的评估与反馈策略，并通过 GraphRAG 提供可追溯的证据引用，最终形成“数据—分析—干预—反馈”的闭环。

\section{国内外研究现状}
学术写作研究与写作教学长期关注写作过程与文本产出之间的关系。写作过程模型（如认知过程理论）强调规划、表达与修订在写作中的循环作用，为“过程性反馈”提供了理论依据\cite{flower1981}。面向研究生的学术写作教学实践则更强调体裁（genre）意识、论证结构与学术表达规范，例如对文献综述、研究计划与论文各部分的功能定位进行训练\cite{swales2012,hyland2015}。与此同时，自动作文评估与写作反馈系统逐步成熟，能够对语法、篇章结构或整体质量给出评分与提示\cite{shermis2013}，但很多系统仍以“单次评分”为主，难以结合学生历史轨迹给出稳定的个性化建议。

在学习分析与学生建模方面，学习管理系统（LMS）为学习过程数据采集提供了基础，但多数分析仍停留在访问次数、学习时长等行为统计层面。对于写作课程而言，真正具有教学价值的信号往往来自文本本身与修改过程，例如结构性问题的反复出现、论证链条缺失、证据引用不规范等。这类信号需要将文本分析结果与学习事件结合，才能形成可解释的能力画像。

在大语言模型辅助写作方面，对话式模型可以生成润色、改写与建议，但其输出质量与一致性受提示词、上下文与模型能力影响较大。当模型缺少课程材料支撑或未被约束时，容易产生“建议看似合理但难以复核”的问题。检索增强生成（RAG）\cite{lewis2020}通过先检索证据再生成，将答案锚定在外部知识之上，是当前提升可追溯性的主流方向；知识图谱\cite{hogan2020}则可将课程中的概念、规范与示例按关系组织，支持图结构扩展与片段引用。在教育场景中，将 RAG 与课程 rubrics、教师复核机制结合，是实现“可控输出”的关键路径。

\section{研究内容与任务要求}
本文围绕《学术规范与论文写作》课程场景的“过程性数据沉淀与个性化辅导”开展研究，并以《电磁场与电磁波》课程任务验证工具调用可行性，主要内容与任务要求如下。

\subsection{课题内容}
\begin{enumerate}[label=(\arabic*)]
  \item 以学生为中心的数据模型与学习事件体系设计，实现写作提交、对话辅导、学习时长等过程性数据的可追踪沉淀。
  \item 写作类型感知的智能分析与反馈策略设计，支持文献综述、课程论文、学位论文、摘要等类型的差异化 rubric 与结构化输出。
  \item GraphRAG 知识库构建与检索增强生成流程实现，使关键建议绑定证据片段并可引用溯源。
  \item 面向跨端的系统原型实现（Web/Mobile），抽取共享 types 与统一 SDK，保证多端一致的 API 契约与体验。
  \item 模型训练与评测管线实现：数据规范、LoRA/QLoRA 微调脚本与回归评测脚本，支撑后续 100k 规模数据的迭代。
\end{enumerate}

\subsection{课题任务要求}
\begin{enumerate}[label=(\arabic*)]
  \item 掌握大语言模型的基本原理与工程化接入方式，能够通过 OpenAI-compatible 接口完成对话、写作分析与结构化输出。
  \item 掌握写作课程场景下的数据组织方法，能够将写作样本、反馈与过程性事件统一建模并支持查询与聚合。
  \item 完成一个可部署的系统原型，覆盖写作提交、分析、反馈与复核的端到端流程，并支持跨端访问。
  \item 实现可追溯的 RAG 辅导能力：当证据不足时能追问或拒答，并避免编造引用。
  \item 完成训练与评测工具链的实现与验证，为后续模型微调提供可复现基础。
\end{enumerate}

\section{创新点}
本文工作的创新点主要体现在以下几个方面。
\begin{enumerate}[label=(\arabic*)]
  \item 面向写作课程构建“学习档案 + 学习事件”的学生中心闭环，强调过程性数据沉淀与纵向追踪，使反馈不仅面向一次作业，更面向能力发展。
  \item 提出写作类型感知的评估与反馈框架，以类型化 rubric 与结构化输出降低反馈的主观性与随机性，提升建议的可执行性。
  \item 引入 GraphRAG 形成可追溯的证据引用链路：将课程规范与范文片段构建为知识库，生成反馈时强制引用片段编号，降低幻觉并便于复核。
  \item 采用跨端共享的 types 与统一 SDK 保障多端一致的 API 契约，将平台能力沉淀为可复用的工程化组件。
\end{enumerate}

\section{论文结构}
本文共分三章：第一章介绍研究背景、相关研究与课题内容；第二章给出需求分析、系统架构与关键模块设计实现，并说明训练与评测管线；第三章总结工作并展望后续改进方向。
